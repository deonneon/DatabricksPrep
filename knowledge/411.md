# Notes

Databricks Community Edition runs on AWS

Databricks academy has provided notebooks

## Cache

Cache is a buffer between app and storage

It is usually stored in memory but could be in disk space that is fast like SSD

Stores frequently asked data or last used data.

### Type of Caching

1. Application-level Cache
   Stores everything - query, compute, objects
2. Database-level Cache
   Stores frequently accessed data blocks
3. Distrbuted Cache
   Spans multiple servers to provide high availability

### Key Caching Strategies

- Write-Through: Data is written to both cache and storage simultaneously
- Write-Back: Data is written to cache first, then asynchronously to storage
- Write-Around: Data is written directly to storage, bypassing cache
- Read-Through: Cache automatically loads missing items from storage

### Cache Eviction Policies

Least Recently Used (LRU): Removes least recently accessed items
Least Frequently Used (LFU): Removes least frequently accessed items
First In First Out (FIFO): Removes oldest items first
Time-based expiration: Removes items after a set time period

### Common Challenges

Cache invalidation: Keeping cache in sync with primary storage
Cache coherence: Maintaining consistency across distributed caches
Cache penetration: Handling requests for non-existent data
Cache avalanche: Managing sudden cache expiration of many items

### Databricks Cache Specific

#### DataFrame/Table Cache

Uses .cache() or CACHE TABLE commands
Stores data in memory and/or disk using the unified Delta cache
Cache is maintained until:

Cluster is terminated
Cache is explicitly unpersisted
Memory pressure forces eviction

```py
# Cache a DataFrame
df = spark.read.table("my_table")
df.cache()

# Or using SQL
spark.sql("CACHE TABLE my_table")
```

#### Delta Cache

Automatically caches data at the file level
Uses a local cache on each executor node

Identifies files using combination of:

- File path
- File version/transaction ID
- File modification timestamp
- Partition information

```py
# Enable Delta Cache
spark.conf.set("spark.databricks.io.cache.enabled", "true")

# Set cache size (default is 50GB)
spark.conf.set("spark.databricks.io.cache.maxSize", "100g")
```
