# Notes

Databricks Community Edition runs on AWS

Databricks academy has provided notebooks

## Cache

Cache is a buffer between app and storage

It is usually stored in memory but could be in disk space that is fast like SSD

Stores frequently asked data or last used data.

### Type of Caching

1. Application-level Cache
   Stores everything - query, compute, objects
2. Database-level Cache
   Stores frequently accessed data blocks
3. Distrbuted Cache
   Spans multiple servers to provide high availability

### Key Caching Strategies

- Write-Through: Data is written to both cache and storage simultaneously
- Write-Back: Data is written to cache first, then asynchronously to storage
- Write-Around: Data is written directly to storage, bypassing cache
- Read-Through: Cache automatically loads missing items from storage

### Cache Eviction Policies

Least Recently Used (LRU): Removes least recently accessed items
Least Frequently Used (LFU): Removes least frequently accessed items
First In First Out (FIFO): Removes oldest items first
Time-based expiration: Removes items after a set time period

### Common Challenges

Cache invalidation: Keeping cache in sync with primary storage
Cache coherence: Maintaining consistency across distributed caches
Cache penetration: Handling requests for non-existent data
Cache avalanche: Managing sudden cache expiration of many items

### Databricks Cache Specific

#### DataFrame/Table Cache

Uses .cache() or CACHE TABLE commands
Stores data in memory and/or disk using the unified Delta cache

Cache is maintained until:
Cluster is terminated
Cache is explicitly unpersisted
Memory pressure forces eviction

```py
# Cache a DataFrame
df = spark.read.table("my_table")
df.cache()

# Or using SQL
spark.sql("CACHE TABLE my_table")
```

#### Delta Cache

Automatically caches data at the file level
Uses a local cache on each executor node

Identifies files using combination of:

- File path
- File version/transaction ID
- File modification timestamp
- Partition information

```py
# Enable Delta Cache
spark.conf.set("spark.databricks.io.cache.enabled", "true")

# Set cache size (default is 50GB)
spark.conf.set("spark.databricks.io.cache.maxSize", "100g")
```

## notebook jobs

you can run Databricks notebooks as production scripts by adding them as a task in a Databricks job.

## Securable objects - Volumes

Databricks uses two primary securable objects to store and access data. Tables govern access to tabular data. Volumes govern access to non-tabular data.

## DBU

Goal: Increase databricks footprint.

DBU increases through optimization, high impact use cases, real-time streaming, extensive tables. compute-heavy workloads
Photon usage, training models.

Unity catalog governance which increase compute

Workflows and automation,CI/CD

Encourage them to adopt AI/ML for predictive maintenance, IoT analytics, supply chain optimization, and quality control.

introduce Delta Live Tables (DLT) for real-time streaming, improving factory floor monitoring.
Push Lakehouse architecture as a replacement for expensive traditional warehouses.
Advocate for data-sharing features with suppliers and partners using Unity Catalog.

Delta Sharing via Unity Catalog
Manufacturers can securely share live data with suppliers, distributors, and partners without creating copies.
Uses open Delta Sharing protocol, so partners don’t need to be on Databricks; they can access data via Snowflake, Pandas, or BI tools like Power BI.
Supports fine-grained permissions at the table, column, and row levels.

## Streaming Data

IoT, PLC (programmable logic controller, some mqtt broker)

## Manufacturing Example Pipeline

A CNC machine in a factory generates telemetry data, including temperature, vibration, and error codes. The goal is to stream this data into Databricks for real-time monitoring, anomaly detection, and predictive maintenance.

Pipeline Flow Summary
1️⃣ Edge device (CNC Machine) → Publishes telemetry data via MQTT
2️⃣ Azure IoT Hub → Ingests messages & routes to Event Hub
3️⃣ Databricks Structured Streaming → Reads & filters real-time data
4️⃣ Delta Lake → Stores time-series sensor data for analytics
5️⃣ ML Model (Databricks MLflow) → Detects anomalies & predicts failures
6️⃣ Databricks SQL / Power BI → Real-time monitoring dashboards

Why This Pipeline?
✅ Scalable: Handles high-frequency sensor data
✅ Low Latency: Near real-time analytics
✅ Fault-Tolerant: Uses Delta Lake & Event Hubs
✅ ML-Ready: Supports predictive maintenance models

## ACID

Databricks way

Atomic - load to commit log first (stage)
Consistency - Schema enforcedment
Isolation - Each read gets a snapshot of table, Multiple concurrent writes do not interfere with each other due to version control and Delta’s conflict resolution mechanism
Durability - file-based storage and commit log
